[thetagpu24:269096] Warning: could not find environment variable "PYTHONPATH"
[1633399517.953641] [thetagpu24:269111:0]    ucp_context.c:735  UCX  WARN  transports 'cuda_copy','cuda_ipc' are not available, please use one or more of: cma, dc, dc_mlx5, dc_x, ib, knem, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x
2021-10-04 21:05:18.537426: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-10-04 21:05:18.537493: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: thetagpu24
2021-10-04 21:05:18.537501: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: thetagpu24
2021-10-04 21:05:18.537583: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.142.0
2021-10-04 21:05:18.537606: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.142.0
2021-10-04 21:05:18.537613: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.142.0
2021-10-04 21:05:18.538085: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-04 21:05:21.489880: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch: 0 	Loss: 184.053 	Time/epoch: 137
Epoch: 1 	Loss: 73.415 	Time/epoch: 133
Epoch: 2 	Loss: 55.557 	Time/epoch: 133
Epoch: 3 	Loss: 46.387 	Time/epoch: 132
Epoch: 4 	Loss: 38.892 	Time/epoch: 132
Epoch: 5 	Loss: 33.116 	Time/epoch: 132
Epoch: 6 	Loss: 31.728 	Time/epoch: 132
Epoch: 7 	Loss: 25.346 	Time/epoch: 132
Epoch: 8 	Loss: 24.299 	Time/epoch: 133
Epoch: 9 	Loss: 22.145 	Time/epoch: 133
Average epoch training time: 132 seconds
