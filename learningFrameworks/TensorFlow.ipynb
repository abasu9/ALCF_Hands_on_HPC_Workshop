{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is an open source machine learning framework developed primarily by Google and released for a variety of languages.  We only focus on Python here, since that is the primary use of TensorFlow on ALCF systems.  For support for other modes, please contact support@alcf.anl.gov.\n",
    "\n",
    "The TensorFlow documentation is here:\n",
    "https://www.tensorflow.org/\n",
    "\n",
    "To get started with TensorFlow, import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow basics\n",
    "\n",
    "### `Tensor`\n",
    "TensorFlow uses the concept of `Tensors` as data types, and supports a variety of operations on them.  This document is not meant to be a [TensorFlow tutorial](https://www.tensorflow.org/tutorials/) - instead, this is meant to inform you of the core concepts of using TensorFlow on Polaris, assuming you have some familiarity with TensorFlow already.\n",
    "\n",
    "You can learn more about tensors in detail here:\n",
    "https://www.tensorflow.org/guide/tensor\n",
    "\n",
    "### GPU Computing\n",
    "\n",
    "TensorFlow supports GPU operations for a large set of mathematical operations on Tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Computing:\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    cpu_input_data = tf.random.uniform(shape=(2,5000,500))\n",
    "\n",
    "    print(\"input data location: \")\n",
    "    print(cpu_input_data.device)\n",
    "    print()\n",
    "    \n",
    "    # This runs on the CPU:\n",
    "    product = tf.linalg.matmul(cpu_input_data, cpu_input_data, transpose_a=True)\n",
    "    print(\"output data location: \")\n",
    "    print(product.device)\n",
    "    print()\n",
    "    \n",
    "    print(cpu_input_data.shape)\n",
    "    print(product.shape)\n",
    "    print()\n",
    "    \n",
    "    # Time the operation\n",
    "    del product\n",
    "    %timeit product = tf.linalg.matmul(cpu_input_data, cpu_input_data, transpose_a=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Computing:\n",
    "\n",
    "with tf.device(\"GPU\"):\n",
    "    gpu_input_data = tf.random.uniform(shape=(2,5000,500))\n",
    "\n",
    "    print(\"input data location: \")\n",
    "    print(gpu_input_data.device)\n",
    "    print()\n",
    "\n",
    "    # This runs on the GPU:\n",
    "    product = tf.linalg.matmul(gpu_input_data, gpu_input_data, transpose_a=True)\n",
    "    print(\"output data location: \")\n",
    "    print(product.device)\n",
    "    print()\n",
    "    \n",
    "    print(gpu_input_data.shape)\n",
    "    print(product.shape)\n",
    "    print()\n",
    "    \n",
    "    # Time the operation\n",
    "    del product\n",
    "    %timeit product = tf.linalg.matmul(gpu_input_data, gpu_input_data, transpose_a=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting access to Data\n",
    "\n",
    "We'll cover the Data Pipelines more completely in a later presentation.  For now, we'll use the [cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), available from TensorFlow. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying to run this on a mac?  Try these lines if you get an SSL error\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# This data has 50000 images, of size 32x32 pixels and 3 RGB colors\n",
    "print(x_train.shape)\n",
    "\n",
    "# with 50000 labels from ten different classes (integers ranging from 0-10)\n",
    "print(y_train.shape)\n",
    "print(y_train[:5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data needs to be converted from numpy arrays to tensor objects\n",
    "print(type(x_train))\n",
    "print(x_train.dtype)\n",
    "\n",
    "print(type(y_train))\n",
    "print(y_train.dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with a batch of 10% of the data\n",
    "batch_size   = 5000\n",
    "batch_data   = tf.convert_to_tensor(x_train[0:batch_size], dtype=tf.float32) # Take the first 10% of images\n",
    "batch_labels = tf.convert_to_tensor(y_train[0:batch_size], dtype=tf.float32) # first 10% of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print()\n",
    "\n",
    "# We explicitly converted form uint8 to float32 so no surprises show up with mathematical operations later on\n",
    "print(batch_data.dtype)\n",
    "print(batch_labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models\n",
    "\n",
    "TensorFlow is primarily developed as a machine learning framework, thus many operations like convolution, dense layers, etc. are all well supported.\n",
    "\n",
    "The easiest way to build a model is to use the [Keras API](https://keras.io/) for object-oriented model construction.  For example, building a few layers of a [ResNet](\n",
    "https://doi.org/10.48550/arXiv.1512.03385)-like model can be done like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Call the parent class's __init__ to make this class functional with training loops:\n",
    "        super().__init__()\n",
    "        self.conv1  = tf.keras.layers.Conv2D(filters=16, kernel_size=[3,3], padding=\"same\")\n",
    "        self.conv2  = tf.keras.layers.Conv2D(filters=16, kernel_size=[3,3], padding=\"same\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "    \n",
    "        # Apply the first weights + activation:\n",
    "        outputs = tf.keras.activations.relu(self.conv1(inputs))\n",
    "        \n",
    "        # Apply the second weights:\n",
    "        outputs = self.conv2(outputs)\n",
    "\n",
    "        # Perform the residual step:\n",
    "        outputs = outputs + inputs\n",
    "\n",
    "        # Second activation layer:\n",
    "        return tf.keras.activations.relu(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Call the parent class's __init__ to make this class functional with training loops:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_init = tf.keras.layers.Conv2D(filters=16, kernel_size=1)\n",
    "        \n",
    "        self.res1 = ResidualBlock()\n",
    "        \n",
    "        self.res2 = ResidualBlock()\n",
    "        \n",
    "        # 10 filters, one for each possible label (classification):\n",
    "        self.conv_final = tf.keras.layers.Conv2D(filters=10, kernel_size=1)\n",
    "        \n",
    "        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv_init(inputs)\n",
    "        \n",
    "        x = self.res1(x)\n",
    "        \n",
    "        x = self.res2(x)\n",
    "        \n",
    "        x = self.conv_final(x)\n",
    "        \n",
    "        return self.pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model:\n",
    "model = MyModel()\n",
    "model.build(batch_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize your networks easily with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "The big advantage of the Machine Learning Frameworks is automatic differentiation.  TensorFlow supports automatic differentiation with the `GradientTape` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin by defining a loss function (see https://www.tensorflow.org/api_docs/python/tf/keras/losses for more built-in loss functions)\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    logits = model(batch_data)\n",
    "    loss = loss_function(batch_labels, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the \"normal\" derivatives (with respect to parameters) with the tape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get gradients of other components by asking the tape to `watch` tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track gradients of the input data\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(batch_data)\n",
    "    logits = model(batch_data)\n",
    "    loss = loss_function(batch_labels, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_grads = tape.gradient(loss, batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_grads.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Functions\n",
    "\n",
    "Often, TensorFlow code can run faster when you [graph compile](https://www.tensorflow.org/guide/intro_to_graphs) it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch_data)\n",
    "        loss = loss_function(batch_labels, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a baseline time for the execution of the gradient step\n",
    "%timeit gradient_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph compile the gradient step\n",
    "gradient_step_traced = tf.function(gradient_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the time to run the graph compiled code\n",
    "%timeit gradient_step_traced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further improvements can be found [with XLA (Accelerated Linear Algebra)](https://www.tensorflow.org/xla):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph compile with XLA\n",
    "gradient_step_XLA = tf.function(gradient_step, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the time to run the graph compiled code with XLA optimizations\n",
    "%timeit gradient_step_XLA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Precision\n",
    "\n",
    "NVIDIA A100 GPUs have support for faster matrix operations with [mixed precision](https://www.tensorflow.org/guide/mixed_precision), which can be enabled in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile using mixed precision policy\n",
    "gradient_step_XLA = tf.function(gradient_step, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the timing of the graph compiled code with XLA optimizations\n",
    "%timeit gradient_step_XLA()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
