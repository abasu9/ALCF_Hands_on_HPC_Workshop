{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch \n",
    "\n",
    "[PyTorch](https://pytorch.org/) is another Machine Learning Framework, similar in many ways to TensorFlow but with a few key differences.\n",
    "\n",
    " - PyTorch does not support `function` compilation in the same way that TensorFlow does\n",
    " - PyTorch generally uses less memory than TensorFlow\n",
    " - PyTorch preserves a more `numpy`-like interface\n",
    " \n",
    " More information about pytorch can be found here: https://pytorch.org/\n",
    " \n",
    " In this short notebook, we'll cover the same topics as before in the [TensorFlow notebook](https://github.com/argonne-lcf/sdl_workshop/blob/learningFrameworks/learningFrameworks/TensorFlow.ipynb), but this time in PyTorch.\n",
    " \n",
    " This document This document is not meant to be a [PyTorch tutorial](https://pytorch.org/tutorials/) - instead, this is meant to inform you of the core concepts of using PyTorch on Polaris, assuming you have some familiarity with PyTorch already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we temporarilgy load TensorFlow in order to import the [cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "del tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we will work with a batch of 10% of the data\n",
    "batch_size=5000\n",
    "batch_data = x_train[0:batch_size].transpose((0,3,1,2)) # permute the axes\n",
    "batch_labels = y_train[0:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(batch_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = torch.Tensor(batch_data)\n",
    "batch_labels = torch.Tensor(batch_labels).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.dtype)\n",
    "print(batch_data.dtype)\n",
    "print()\n",
    "\n",
    "print(y_train.dtype)\n",
    "print(batch_labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Machine Learning Models\n",
    "\n",
    "PyTorch's `nn` package allows an object-oriented way to create models, just like Keras in TensorFlow. There is also a [functional API](https://pytorch.org/docs/stable/index.html) that works similarily. For example, building a few layers of a [ResNet](https://doi.org/10.48550/arXiv.1512.03385)-like model can be done like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Call the parent class's __init__ to make this class functional with training loops:\n",
    "        super().__init__()\n",
    "        self.conv1  = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=[3,3], padding=[1,1])\n",
    "        self.conv2  = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=[3,3], padding=[1,1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "    \n",
    "        # Apply the first weights + activation:\n",
    "        outputs = torch.nn.functional.relu(self.conv1(inputs))\n",
    "        \n",
    "        # Apply the second weights:\n",
    "        outputs = self.conv2(outputs)\n",
    "\n",
    "        # Perform the residual step:\n",
    "        outputs = outputs + inputs\n",
    "\n",
    "        # Second activation layer:\n",
    "        return torch.nn.functional.relu(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Call the parent class's __init__ to make this class functional with training loops:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_init = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=1)\n",
    "        \n",
    "        self.res1 = ResidualBlock()\n",
    "        \n",
    "        self.res2 = ResidualBlock()\n",
    "        \n",
    "        # 10 filters, one for each possible label (classification):\n",
    "        self.conv_final = torch.nn.Conv2d(in_channels=16, out_channels=10, kernel_size=1)\n",
    "        \n",
    "        self.pool = torch.nn.AvgPool2d(32,32)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.conv_init(inputs)\n",
    "        \n",
    "        x = self.res1(x)\n",
    "        \n",
    "        x = self.res2(x)\n",
    "        \n",
    "        x = self.conv_final(x)\n",
    "        \n",
    "        return self.pool(x).reshape((-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "_num_trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of Trainable Parameters: {:d}\".format(_num_trainable_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "The big advantage of the Machine Learning Frameworks is automatic differentiation.  PyTorch supports automatic differentiation through the automatic differentiation package the `torch.autograd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(batch_data)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits, batch_labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = torch.autograd.grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(model.parameters()):\n",
    "    print(gradients[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_grads = torch.autograd.grad(loss, batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(batch_data.requires_grad_())\n",
    "loss = torch.nn.functional.cross_entropy(logits, batch_labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_grads = torch.autograd.grad(loss, batch_data)[0] # <-- returns tuple with input gradients as only member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_data.shape)\n",
    "print(input_grads.shape)\n",
    "\n",
    "print(input_grads[0,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Performance\n",
    "\n",
    "Here's the same gradient step function using an identical model that was in the TensorFlow notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step():\n",
    "    logits = model(batch_data)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, batch_labels.flatten())\n",
    "    gradients = torch.autograd.grad(loss, model.parameters())\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gradient_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it is significantly slower.  However, for larger input sizes and models PyTorch is quite competitive with TensorFlow, and sometimes faster.  PyTorch also has JIT functionality, but it does not make the same improvements as TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_module = torch.jit.trace_module(model, inputs={\"forward\" : batch_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit traced_module(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit model(batch_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
