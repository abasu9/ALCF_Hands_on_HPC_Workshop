# LLM Inference Optimizations


* **Time:** October 30, 2024. 
  * 11.30 - 12.30 : LLM Inference Optimizations
* **Location:** TCS 1416 
* **Slack Channel:** #infr-opti : Use to post questions, exact error messages etc. 

## Agenda

* Motivational Example: Download model weights from HF and serve that model with custom queries using HF TGI.
* Paged Attention: Re-do the same example using vLLM
* Parallelism: Use TensorRT-LLM for TP, PP and EP. 
* KV-Cache: Use one of the techniques like H2O or Streaming LLM to show benefits. 


## Useful Links 

+ [Link to Presentation Slides]()
+ [ALCF Hands-on  HPC Workshop Agenda](https://www.alcf.anl.gov/events/2024-alcf-hands-hpc-workshop)
+ [vLLM Documentation](https://docs.vllm.ai/en/latest/)
+ [vLLM Repo](https://github.com/vllm-project/vllm)
+ [TensorRT-LLM Documentation](https://nvidia.github.io/TensorRT-LLM/)
+ [TensorRT-LLM Repo](https://github.com/NVIDIA/TensorRT-LLM/tree/main)


