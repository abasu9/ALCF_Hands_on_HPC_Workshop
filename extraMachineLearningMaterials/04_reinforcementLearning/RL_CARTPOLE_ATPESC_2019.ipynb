{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATPESC 2019 REINFORCEMENT LEARNING TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented by Sami Khairy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial Outline\n",
    "In this tutorial we will solve the CartPole-v1 classic control problem, using three RL algorithms:\n",
    "1. Tabular Q-Learning\n",
    "2. Deep Q-Networks (DQN)\n",
    "3. Simple Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Description\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\"\n",
    "\n",
    "Source: https://gym.openai.com/envs/CartPole-v1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import necessary packages\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CartPoleEnv in module gym.envs.classic_control.cartpole object:\n",
      "\n",
      "class CartPoleEnv(gym.core.Env)\n",
      " |  Description:\n",
      " |      A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
      " |  \n",
      " |  Source:\n",
      " |      This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
      " |  \n",
      " |  Observation: \n",
      " |      Type: Box(4)\n",
      " |      Num     Observation                 Min         Max\n",
      " |      0       Cart Position             -4.8            4.8\n",
      " |      1       Cart Velocity             -Inf            Inf\n",
      " |      2       Pole Angle                 -24 deg        24 deg\n",
      " |      3       Pole Velocity At Tip      -Inf            Inf\n",
      " |      \n",
      " |  Actions:\n",
      " |      Type: Discrete(2)\n",
      " |      Num     Action\n",
      " |      0       Push cart to the left\n",
      " |      1       Push cart to the right\n",
      " |      \n",
      " |      Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
      " |  \n",
      " |  Reward:\n",
      " |      Reward is 1 for every step taken, including the termination step\n",
      " |  \n",
      " |  Starting State:\n",
      " |      All observations are assigned a uniform random value in [-0.05..0.05]\n",
      " |  \n",
      " |  Episode Termination:\n",
      " |      Pole Angle is more than 12 degrees\n",
      " |      Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n",
      " |      Episode length is greater than 200\n",
      " |      Solved Requirements\n",
      " |      Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CartPoleEnv\n",
      " |      gym.core.Env\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override _close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      environments do not support rendering at all.) By convention,\n",
      " |      if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render.modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode is 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Resets the state of the environment and returns an initial observation.\n",
      " |      \n",
      " |      Returns: observation (object): the initial observation of the\n",
      " |          space.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  step(self, action)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the environment\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  action_space = None\n",
      " |  \n",
      " |  observation_space = None\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env.env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Action Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "episode_cnt = 0\n",
    "episode_ret = 0\n",
    "time_steps = 0\n",
    "episode_returns_random = []\n",
    "num_episodes = 200\n",
    "while episode_cnt < num_episodes:\n",
    "    # env.render()\n",
    "    # sample random actions from your action space\n",
    "    action = env.action_space.sample() \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time_steps += 1\n",
    "    episode_ret += reward\n",
    "    if done:\n",
    "        #Reached a terminal state\n",
    "        observation = env.reset()\n",
    "        episode_cnt += 1\n",
    "        episode_returns_random.append(episode_ret)\n",
    "        episode_ret = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Random Policy.\n",
      "Number of Test Episodes 200.\n",
      "Expected Episode Length: 23.165.\n",
      "Expected Episode Return: 23.165.\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of Random Policy.\")\n",
    "print(\"Number of Test Episodes {}.\".format(episode_cnt))\n",
    "print(\"Expected Episode Length: {}.\".format(time_steps/episode_cnt))\n",
    "print(\"Expected Episode Return: {}.\".format(np.mean(episode_returns_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The state space of CartPole-v1 is a vector in $R^4$. In order to use a tabular Q-learning method, we need to discretize the state space. \n",
    "\n",
    "In order to have a finite number of discrete states, I map the R^4 range into [-1,1]^4 using tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes the state in R^4, maps it to [-1,1]^4 by applying tanh, discretize the space,\n",
    "#and returns index of the state in multidimensional space\n",
    "def state_to_multi_index(state,step_size=0.2):\n",
    "    state = np.tanh(np.asarray(state))\n",
    "    assert np.sum(-1 <= state)+ np.sum(1 >= state) == 2*len(state)\n",
    "    positive_state = state + 1\n",
    "    state_index = []\n",
    "    for s_dim in positive_state:\n",
    "        state_index.append(int(np.ceil(s_dim/step_size)))\n",
    "    return state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 20, 19, 20, 1]\n"
     ]
    }
   ],
   "source": [
    "#example,\n",
    "s_temp = np.asarray([-10,3, 1.1,2.1, -2])\n",
    "print(state_to_multi_index(state=s_temp,step_size=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function converts the index in a multidimensional array to a linear index, so we can have a 2D Q-table later on\n",
    "def convert(s, step_size=0.2):\n",
    "    s_id = state_to_multi_index(s,step_size)\n",
    "    max_l = 2/step_size + 1\n",
    "    assert np.sum(0 <= np.asarray(s_id))+ np.sum(max_l >= np.asarray(s_id)) == 2*len(s)\n",
    "    linear_index = 0\n",
    "    for i in range(len(s)):\n",
    "        linear_index += s_id[i]*(max_l**i)\n",
    "    return int(linear_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_l = 2/step_size +1\n",
    "number_of_states = int(max_l**env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194481"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:05<00:00, 382.63it/s]\n"
     ]
    }
   ],
   "source": [
    "#Initialize Q-table with all zeros\n",
    "Q = np.zeros([number_of_states,env.action_space.n])\n",
    "#Set learning parameters\n",
    "lr = .8\n",
    "gamma = .95\n",
    "num_episodes = 2000\n",
    "#list to contain total rewards and steps per episode\n",
    "ep_len = []\n",
    "ep_ret = []\n",
    "epsilon = 1\n",
    "for i in tqdm(range(num_episodes)):\n",
    "    #Reset environment and get first state\n",
    "    s_ = env.reset()\n",
    "    s = convert(s_,step_size)\n",
    "    curr_ep_ret = 0\n",
    "    terminal = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 500:\n",
    "        j+=1\n",
    "        #Choose an action using an epsilon-greedy approach\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = 1 if np.random.uniform(0,1) >= 0.5 else 0\n",
    "        else:\n",
    "            a = np.argmax(Q[s,:])\n",
    "\n",
    "        #Get new state and reward from environment\n",
    "        s1_,r,terminal,_ = env.step(a)\n",
    "        s1 = convert(s1_)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s1,:]) - Q[s,a])\n",
    "        curr_ep_ret += r\n",
    "        s = s1\n",
    "        \n",
    "        if terminal == True:\n",
    "            #decay epsilon over time. we need to be greedy in the limit\n",
    "            epsilon = epsilon*0.9995\n",
    "            break\n",
    "    ep_len.append(j)\n",
    "    ep_ret.append(curr_ep_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "observation = convert(observation)\n",
    "episode_cnt = 0\n",
    "episode_ret = 0\n",
    "time_steps = 0\n",
    "episode_returns_qtable = []\n",
    "num_episodes = 200\n",
    "while episode_cnt < num_episodes:\n",
    "    # env.render()\n",
    "    # Get action as given by your Q-table (function)\n",
    "    action = np.argmax(Q[observation,:])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    observation = convert(observation)\n",
    "    time_steps += 1\n",
    "    episode_ret += reward\n",
    "    if done:\n",
    "        #Reached a terminal state\n",
    "        observation = env.reset()\n",
    "        observation = convert(observation)\n",
    "        episode_cnt += 1\n",
    "        episode_returns_qtable.append(episode_ret)\n",
    "        episode_ret = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning training performance after 2000 episodes of training.\n",
      "Number of Test Episodes 200.\n",
      "Expected Episode Length: 67.485.\n",
      "Expected Episode Return: 67.485.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q-learning training performance after 2000 episodes of training.\")\n",
    "print(\"Number of Test Episodes {}.\".format(episode_cnt))\n",
    "print(\"Expected Episode Length: {}.\".format(time_steps/episode_cnt))\n",
    "print(\"Expected Episode Return: {}.\".format(np.mean(episode_returns_qtable)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE = 2000 # max number of training episodes\n",
    "MAX_STEP = 200 # max number of steps in an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters for DQN\n",
    "GAMMA = 0.9 # discount factor \n",
    "EPSILON_MAX = 0.5 # starting value of epsilon\n",
    "EPSILON_MIN = 0.01 # final value of epsilon\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "BATCH_SIZE = 32 # size of minibatch\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque()\n",
    "time_step = 0\n",
    "epsilon = EPSILON_MAX\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Q-Network\n",
    "hidden_neurons = 24\n",
    "state_input = tf.placeholder(\"float\",[None,state_dim])\n",
    "hidden_layer = tf.layers.dense(state_input, units=hidden_neurons, activation=tf.nn.relu)\n",
    "hidden_layer = tf.layers.dense(hidden_layer, units=hidden_neurons, activation=tf.nn.relu)\n",
    "Q_value = tf.layers.dense(hidden_layer, units=action_dim, activation=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_input = tf.placeholder(\"float\",[None,action_dim]) # one hot encoding\n",
    "y_input = tf.placeholder(\"float\",[None])\n",
    "Q_action = tf.reduce_sum(tf.math.multiply(Q_value,action_input),reduction_indices = 1)\n",
    "loss = tf.reduce_mean(tf.square(y_input - Q_action))\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skhairy/anaconda3/envs/qopt/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:38<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#training the agent\n",
    "for episode in tqdm(range(MAX_EPISODE)):\n",
    "    # Get initial State\n",
    "    state = env.reset()\n",
    "    # Train \n",
    "    for step in range(MAX_STEP):\n",
    "        \n",
    "        #Get Q(s,)\n",
    "        Q_value_t = Q_value.eval(feed_dict = {state_input:[state]})[0]\n",
    "        #Epsilon-Greedy action selection strategy\n",
    "        if random.random() <= epsilon:\n",
    "            action =  random.randint(0,action_dim - 1)\n",
    "        else:\n",
    "            action = np.argmax(Q_value_t)\n",
    "\n",
    "        #Decay Epsilon over time\n",
    "        epsilon -= (EPSILON_MAX - EPSILON_MIN)/MAX_EPISODE\n",
    "\n",
    "    \n",
    "        next_state,reward,done_ep,_ = env.step(action)\n",
    "        # Define reward for agent\n",
    "        reward_agent = -1 if done_ep else 0.1\n",
    "        \n",
    "        one_hot_action = np.zeros(action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        \n",
    "        #store most recent interaction\n",
    "        replay_buffer.append((state,one_hot_action,reward,next_state,done_ep))\n",
    "        #if buffer is full, remove oldest interaction\n",
    "        if len(replay_buffer) > REPLAY_SIZE:\n",
    "            replay_buffer.popleft()\n",
    "\n",
    "        if len(replay_buffer) > BATCH_SIZE:\n",
    "            time_step += 1\n",
    "            # Step 1: obtain random minibatch from replay memory\n",
    "            minibatch = random.sample(replay_buffer,BATCH_SIZE)\n",
    "            state_batch = [data[0] for data in minibatch]\n",
    "            action_batch = [data[1] for data in minibatch]\n",
    "            reward_batch = [data[2] for data in minibatch]\n",
    "            next_state_batch = [data[3] for data in minibatch]\n",
    "            # Step 2: calculate y\n",
    "            y_batch = []\n",
    "            Q_value_batch = Q_value.eval(feed_dict={state_input:next_state_batch})\n",
    "            for i in range(0,BATCH_SIZE):\n",
    "                done = minibatch[i][4]\n",
    "                if done:\n",
    "                    y_batch.append(reward_batch[i])\n",
    "                else :\n",
    "                    y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n",
    "\n",
    "            optimizer.run(feed_dict={\n",
    "                                    y_input:y_batch,\n",
    "                                    action_input:action_batch,\n",
    "                                    state_input:state_batch\n",
    "                                    })\n",
    "        \n",
    "        \n",
    "        state = next_state\n",
    "        if done_ep:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING LEARNED POLICY\n",
    "observation = env.reset()\n",
    "episode_cnt = 0\n",
    "episode_ret = 0\n",
    "time_steps = 0\n",
    "episode_returns_DQN = []\n",
    "num_episodes = 200\n",
    "while episode_cnt < num_episodes:\n",
    "    # env.render()\n",
    "    # Get action as given by our learned policy\n",
    "    action = np.argmax(Q_value.eval(feed_dict = {state_input:[observation]})[0])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time_steps += 1\n",
    "    episode_ret += reward\n",
    "    if done:\n",
    "        #Reached a terminal state\n",
    "        observation = env.reset()\n",
    "        episode_cnt += 1\n",
    "        episode_returns_DQN.append(episode_ret)\n",
    "        episode_ret = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN performance after 2000 episodes of training.\n",
      "Number of Test Episodes 200.\n",
      "Expected Episode Length :179.51.\n",
      "Expected Episode Return :179.51.\n"
     ]
    }
   ],
   "source": [
    "print(\"DQN performance after 2000 episodes of training.\")\n",
    "print(\"Number of Test Episodes {}.\".format(episode_cnt))\n",
    "print(\"Expected Episode Length :{}.\".format(time_steps/episode_cnt))\n",
    "print(\"Expected Episode Return :{}.\".format(np.mean(episode_returns_DQN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# number of hidden layer neurons\n",
    "hidden_neurons = 10 \n",
    "# every how many episodes we do parameter update\n",
    "batch_size = 5 \n",
    "learning_rate = 1e-2 \n",
    "gamma = 0.95 \n",
    "#State dimensionality\n",
    "SD = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the policy network, which maps from states to actions.\n",
    "observations_ph = tf.placeholder(tf.float32, [None,SD] , name=\"states\")\n",
    "hidden_layer = tf.layers.dense(observations_ph, units=hidden_neurons, activation=tf.nn.relu)\n",
    "probability = tf.layers.dense(hidden_layer,units=1,activation=tf.nn.sigmoid)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "action_neurons = int(np.math.log(env.action_space.n,2))\n",
    "action_ph = tf.placeholder(tf.float32,[None,action_neurons], name=\"actions\")\n",
    "advantages_ph = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# The loss function. loglikelihood of policy * advantage\n",
    "# actions with good advantage are more likely, and actions without good advantage are less likely.\n",
    "loglik = tf.log(action_ph*(action_ph - probability) + (1 - action_ph)*(action_ph + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages_ph) \n",
    "\n",
    "train_pi = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:15<00:00, 125.75it/s]\n"
     ]
    }
   ],
   "source": [
    "obs_buf,reward_buf,act_buf = [],[],[]\n",
    "running_reward = None\n",
    "episode_ret = 0\n",
    "episode_number = 1\n",
    "total_episodes = 2000\n",
    "episode_returns = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    observation = env.reset() \n",
    "    \n",
    "    pbar = tqdm(total = total_episodes)\n",
    "    while episode_number <= total_episodes: \n",
    "        x = np.reshape(observation,[1,SD])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability,feed_dict={observations_ph: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        obs_buf.append(x) \n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        act_buf.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode_ret += reward\n",
    "\n",
    "        reward_buf.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            pbar.update(1)\n",
    "            # stack inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(obs_buf)\n",
    "            epy = np.vstack(act_buf)\n",
    "            epr = np.vstack(reward_buf)\n",
    "          \n",
    "            # reset array memory\n",
    "            obs_buf,reward_buf,act_buf = [],[],[]\n",
    "            \n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # Normalize discounted rewards to be 0 mean, unity variance. This lowers the gradient estimator variance.\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr //= np.std(discounted_epr)\n",
    "            episode_returns.append(episode_ret)\n",
    "\n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                #launch one training step\n",
    "                sess.run(train_pi,feed_dict={observations_ph: epx, action_ph: epy, advantages_ph: discounted_epr})\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "            if running_reward is None:\n",
    "                running_reward = episode_ret\n",
    "            else:\n",
    "                running_reward = running_reward * 0.99 + episode_ret * 0.01\n",
    "                if running_reward > 199: \n",
    "                    print(\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "            \n",
    "            #if (episode_number%50):\n",
    "            #    print(\"Completed {} Episodes. Running Reward {}\".format(episode_number,running_reward))         \n",
    "            \n",
    "            episode_ret = 0\n",
    "            observation = env.reset()\n",
    "            \n",
    "    pbar.close()\n",
    "    #TESTING LEARNED POLICY\n",
    "    observation = env.reset()\n",
    "    episode_cnt = 0\n",
    "    episode_ret = 0\n",
    "    time_steps = 0\n",
    "    episode_returns_policygrad = []\n",
    "    num_episodes = 200\n",
    "    while episode_cnt < num_episodes:\n",
    "        # env.render()\n",
    "        # Get action as given by our learned policy\n",
    "        x = np.reshape(observation,[1,SD])\n",
    "        tfprob = sess.run(probability,feed_dict={observations_ph: x})\n",
    "        action = 1 if 0.5 < tfprob else 0\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        time_steps += 1\n",
    "        episode_ret += reward\n",
    "        if done:\n",
    "            #Reached a terminal state\n",
    "            observation = env.reset()\n",
    "            episode_cnt += 1\n",
    "            episode_returns_policygrad.append(episode_ret)\n",
    "            episode_ret = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Policy Gradient performance after 2000 episodes of training.\n",
      "Number of Test Episodes 200.\n",
      "Expected Episode Length :447.945.\n",
      "Expected Episode Return :447.945.\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Policy Gradient performance after 2000 episodes of training.\")\n",
    "print(\"Number of Test Episodes {}.\".format(episode_cnt))\n",
    "print(\"Expected Episode Length :{}.\".format(time_steps/episode_cnt))\n",
    "print(\"Expected Episode Return :{}.\".format(np.mean(episode_returns_policygrad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qopt",
   "language": "python",
   "name": "qopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
