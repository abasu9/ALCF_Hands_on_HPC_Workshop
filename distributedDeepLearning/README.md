# Distributed Deep Learning

Led by Huihuo Zheng, Corey Adams, and Zhen Xie from ALCF

This section of the workshop will introduce to you the methods we use to run distributed deep learning training on ALCF resources like Theta and ThetaGPU.

We show distributed training using three frameworks: 
1. [Horovod](Horovod/README.md) (for [TensorFlow](tensorflow.org) and [PyTorch](pytorch.org)), and
2. [DistributedDataParallel](DDP/README.md) (DDP) (for PyTorch only).
3. [DeepSpeed](DeepSpeed/README.md)